{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-03T22:17:01.333668Z",
     "start_time": "2020-11-03T22:17:01.331186Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "what is also practical if you want to have a second look at the tweets as they are saved in twitter: advanced search tool of twitter: https://twitter.com/search-advanced?lang=en\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T11:35:12.193318Z",
     "start_time": "2020-12-23T11:35:12.134663Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-2-ffbd8e905850>:31: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    }
   ],
   "source": [
    "# Base and Cleaning \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import sys\n",
    "\n",
    "#Natural Language Processing (NLP)\n",
    "import spacy     # check out \n",
    "from spacy.tokenizer import Tokenizer\n",
    "import gensim\n",
    "from gensim.corpora import Dictionary # check out \n",
    "from gensim import models\n",
    "from gensim.models.ldamulticore import LdaMulticore\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.parsing.preprocessing import STOPWORDS as SW\n",
    "from wordcloud import STOPWORDS # what is that? \n",
    "stopwords = set(STOPWORDS)\n",
    "\n",
    "#Visualizations\n",
    "import plotly.express as px\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt \n",
    "import pyLDAvis.gensim\n",
    "import chart_studio\n",
    "import chart_studio.plotly as py \n",
    "import chart_studio.tools as tls\n",
    "\n",
    "\n",
    "# settings for easier coding and inspection of data  \n",
    "%matplotlib notebook\n",
    "pd.set_option('display.max_colwidth', -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T11:35:12.438152Z",
     "start_time": "2020-12-23T11:35:12.435193Z"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')#\"error\", \"ignore\", \"always\", \"default\", \"module\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T11:36:24.772103Z",
     "start_time": "2020-12-23T11:36:24.393669Z"
    }
   },
   "outputs": [],
   "source": [
    "parser = lambda x: datetime.strptime(x,'%Y-%m-%d %X') \n",
    "df_import = pd.read_csv(\"full_tweets_without_rt.xlsx\",\n",
    "                             index_col=0,parse_dates =['created_at'], date_parser=parser)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T11:36:26.351682Z",
     "start_time": "2020-12-23T11:36:26.330016Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1340367556947943425</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-19 18:45:03</td>\n",
       "      <td>A $20,000 fee to change a wedding date? New bill aims to protect people from 'bad actor catering halls' https://t.co/57rja8kptk https://t.co/Yy6u10D1xf</td>\n",
       "      <td>2020-12-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340329823282016256</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-19 16:15:06</td>\n",
       "      <td>U.S. intelligence agencies are increasingly focused on domestic extremists. Their latest target: Satanists https://t.co/pBUJGL0ZxQ https://t.co/Up0XpgKt9z</td>\n",
       "      <td>2020-12-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340307168923414531</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-19 14:45:05</td>\n",
       "      <td>Rebuilding trust in the Justice Department starts — but doesn't end — with Biden's choice of attorney general https://t.co/JriZ0YmQKl https://t.co/mvCEfyziJV</td>\n",
       "      <td>2020-12-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340141062724194306</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-19 03:45:02</td>\n",
       "      <td>Biden says Lindsey Graham is a 'personal disappointment' for not recognizing him as president-elect https://t.co/HONKs8lHru https://t.co/5XDdxh3L5b</td>\n",
       "      <td>2020-12-19 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340065561523458048</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-18 22:45:01</td>\n",
       "      <td>75 people and counting test positive for COVID after North Carolina church’s Christmas musical https://t.co/Xk7u4lMnke https://t.co/PsCfvK8KBM</td>\n",
       "      <td>2020-12-18 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316355491946213376</th>\n",
       "      <td>mailonline</td>\n",
       "      <td>2020-10-14 12:29:40</td>\n",
       "      <td>Your dog has a mid-life crisis too! Pet pooches 'get bored of life and less excited by new experiences when they hit middle age' https://t.co/0F7y1DI7ZH</td>\n",
       "      <td>2020-10-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316314715434430464</th>\n",
       "      <td>mailonline</td>\n",
       "      <td>2020-10-14 09:47:38</td>\n",
       "      <td>Thai pro-democracy protesters fight with royalists on streets of Bangkok amid calls to curb powers of billionaire king https://t.co/bQwqlTOq6w https://t.co/qrhX3DNhyz</td>\n",
       "      <td>2020-10-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316309906010583045</th>\n",
       "      <td>mailonline</td>\n",
       "      <td>2020-10-14 09:28:32</td>\n",
       "      <td>Fisherman is filmed holding dolphin calf underwater before dragging it under tarpaulin to be killed in Japan's Red Cove https://t.co/nFFrEpLNyP</td>\n",
       "      <td>2020-10-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316291670217052166</th>\n",
       "      <td>mailonline</td>\n",
       "      <td>2020-10-14 08:16:04</td>\n",
       "      <td>Thai pro-democracy protesters fight with royalists on streets of Bangkok amid calls to curb powers of billionaire king https://t.co/MxguCGd0xp</td>\n",
       "      <td>2020-10-14 00:00:00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1316036386554884096</th>\n",
       "      <td>mailonline</td>\n",
       "      <td>2020-10-13 15:21:39</td>\n",
       "      <td>12,000lb RAF 'Tallboy' bomb EXPLODES while being defused underwater by military divers at Polish port https://t.co/QgSwsvzRtP https://t.co/yU4B1yiWuX</td>\n",
       "      <td>2020-10-13 00:00:00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20796 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                         agency          created_at  \\\n",
       "1340367556947943425  yahoonews  2020-12-19 18:45:03   \n",
       "1340329823282016256  yahoonews  2020-12-19 16:15:06   \n",
       "1340307168923414531  yahoonews  2020-12-19 14:45:05   \n",
       "1340141062724194306  yahoonews  2020-12-19 03:45:02   \n",
       "1340065561523458048  yahoonews  2020-12-18 22:45:01   \n",
       "...                        ...                  ...   \n",
       "1316355491946213376  mailonline 2020-10-14 12:29:40   \n",
       "1316314715434430464  mailonline 2020-10-14 09:47:38   \n",
       "1316309906010583045  mailonline 2020-10-14 09:28:32   \n",
       "1316291670217052166  mailonline 2020-10-14 08:16:04   \n",
       "1316036386554884096  mailonline 2020-10-13 15:21:39   \n",
       "\n",
       "                                                                                                                                                                                       text  \\\n",
       "1340367556947943425  A $20,000 fee to change a wedding date? New bill aims to protect people from 'bad actor catering halls' https://t.co/57rja8kptk https://t.co/Yy6u10D1xf                  \n",
       "1340329823282016256  U.S. intelligence agencies are increasingly focused on domestic extremists. Their latest target: Satanists https://t.co/pBUJGL0ZxQ https://t.co/Up0XpgKt9z               \n",
       "1340307168923414531  Rebuilding trust in the Justice Department starts — but doesn't end — with Biden's choice of attorney general https://t.co/JriZ0YmQKl https://t.co/mvCEfyziJV            \n",
       "1340141062724194306  Biden says Lindsey Graham is a 'personal disappointment' for not recognizing him as president-elect https://t.co/HONKs8lHru https://t.co/5XDdxh3L5b                      \n",
       "1340065561523458048  75 people and counting test positive for COVID after North Carolina church’s Christmas musical https://t.co/Xk7u4lMnke https://t.co/PsCfvK8KBM                           \n",
       "...                                                                                                                                                             ...                           \n",
       "1316355491946213376  Your dog has a mid-life crisis too! Pet pooches 'get bored of life and less excited by new experiences when they hit middle age' https://t.co/0F7y1DI7ZH                 \n",
       "1316314715434430464  Thai pro-democracy protesters fight with royalists on streets of Bangkok amid calls to curb powers of billionaire king https://t.co/bQwqlTOq6w https://t.co/qrhX3DNhyz   \n",
       "1316309906010583045  Fisherman is filmed holding dolphin calf underwater before dragging it under tarpaulin to be killed in Japan's Red Cove https://t.co/nFFrEpLNyP                          \n",
       "1316291670217052166  Thai pro-democracy protesters fight with royalists on streets of Bangkok amid calls to curb powers of billionaire king https://t.co/MxguCGd0xp                           \n",
       "1316036386554884096  12,000lb RAF 'Tallboy' bomb EXPLODES while being defused underwater by military divers at Polish port https://t.co/QgSwsvzRtP https://t.co/yU4B1yiWuX                    \n",
       "\n",
       "                                    date  \n",
       "1340367556947943425  2020-12-19 00:00:00  \n",
       "1340329823282016256  2020-12-19 00:00:00  \n",
       "1340307168923414531  2020-12-19 00:00:00  \n",
       "1340141062724194306  2020-12-19 00:00:00  \n",
       "1340065561523458048  2020-12-18 00:00:00  \n",
       "...                                  ...  \n",
       "1316355491946213376  2020-10-14 00:00:00  \n",
       "1316314715434430464  2020-10-14 00:00:00  \n",
       "1316309906010583045  2020-10-14 00:00:00  \n",
       "1316291670217052166  2020-10-14 00:00:00  \n",
       "1316036386554884096  2020-10-13 00:00:00  \n",
       "\n",
       "[20796 rows x 4 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA - Exploratory Data Analysis \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T11:36:58.964271Z",
     "start_time": "2020-12-23T11:36:58.953375Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Electoral College meets Monday to cast votes for president and vice president, completing another key part of the election process that will eventually make Joe Biden the commander-in-chief next year. https://t.co/GISypasijF \n",
      "\n",
      "President Trump has four events planned in Pennsylvania today.\n",
      "https://t.co/U7zhsltCWO \n",
      "\n",
      "“They’re not helping my family, they’re showing disrespect,” Wallace Sr. said. https://t.co/gdioYrwhRq \n",
      "\n",
      "Cheese, nuts and leafy vegetables could help ward off asthma, emphysema and even common colds https://t.co/NsKGpq7TZs \n",
      "\n",
      "The Supreme Court agreed to hear a case on whether the Trump administration can exclude undocumented immigrants from the calculations it will use in apportioning congressional seats.\n",
      "https://t.co/9bVjgLl68R \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None, None, None, None, None]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# eda has also been performed in excel \n",
    "temp = df_import.sample(5)\n",
    "[print(tweet,\"\\n\") for tweet in temp.text.values]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insights gained \n",
    "\n",
    "\n",
    "- there are many links in the tweets \n",
    "- unusual chars in written text\n",
    "- trump, donald, donald trump are just examples of one rubric of synonyms that we want to consider as one for the topic features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - 01 Cleaning of the text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:45:08.954992Z",
     "start_time": "2020-12-23T07:45:08.951820Z"
    }
   },
   "outputs": [],
   "source": [
    "import emoji  # needs to be installed with pip\n",
    "import regex # needs to be installed with pip\n",
    "import re\n",
    "import string\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:45:08.964839Z",
     "start_time": "2020-12-23T07:45:08.958334Z"
    }
   },
   "outputs": [],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    \"\"\"\n",
    "    Removes emoji's from tweets\n",
    "    Accepts:\n",
    "        Text (tweets)\n",
    "    Returns:\n",
    "        Text (emoji free tweets)\n",
    "    \"\"\"\n",
    "    emoji_list = [c for c in text if c in emoji.UNICODE_EMOJI]\n",
    "    if not emoji_list==[] :\n",
    "        print(\"\",emoji_list,end=\"\")\n",
    "    clean_text = ' '.join([str for str in text.split() if not any(i in str for i in emoji_list)])\n",
    "    return clean_text\n",
    "\n",
    "def url_free_text(text):   \n",
    "    '''\n",
    "    Cleans text from urls\n",
    "    '''\n",
    "    text, _ = re.subn(r'http\\S+', '', text)\n",
    "    return text \n",
    "\n",
    "def url_free_text_s(text):  # as some https were missed out on by the previous function\n",
    "    '''\n",
    "    Cleans text from urls\n",
    "    '''\n",
    "    text, _ = re.subn(r'https\\S+', '', text)\n",
    "    return text  \n",
    "\n",
    "\n",
    "# Apply the function above and get tweets free of emoji's\n",
    "call_emoji_free = lambda x: give_emoji_free_text(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:45:09.833564Z",
     "start_time": "2020-12-23T07:45:08.966928Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yahoonews s tweets included the following emojis :\n",
      "\n",
      "\n",
      "huffpost s tweets included the following emojis :\n",
      " ['👇'] ['👀'] ['👌']\n",
      "\n",
      "cnn s tweets included the following emojis :\n",
      " ['📩'] ['📩'] ['📩'] ['📩'] ['📩'] ['🎧'] ['🎧'] ['📩'] ['📩'] ['🎧'] ['📩'] ['📩'] ['📩'] ['▪', '▪', '▪', '▪'] ['📩'] ['📺', '📲', '🗨'] ['📩'] ['📩'] ['▪', '▪', '▪'] ['▪', '▪', '▪'] ['📩'] ['▪', '▪', '▪'] ['📩'] ['▪'] ['🎧'] ['🎧'] ['🎧'] ['📩'] ['📩'] ['🎧'] ['📩'] ['📩'] ['📩'] ['▪', '▪', '▪'] ['▪', '▪'] ['📩'] ['🎧'] ['📩'] ['⚡'] ['📩'] ['🎧'] ['📩'] ['🎧'] ['📩'] ['📩'] ['🎧'] ['▪', '▪', '▪', '▪', '▪', '▪'] ['▪', '▪', '▪', '▪', '▪', '▪'] ['🎃'] ['📩'] ['📩'] ['📩']\n",
      "\n",
      "nytimes s tweets included the following emojis :\n",
      " ['🎄'] ['✨', '✨'] ['🎧'] ['👶'] ['🐀', '👨', '🍳', '🇫', '🇷'] ['🌕'] ['✨', '✨'] ['🦘', '💬', '🧍', '♂'] ['👇'] ['🎶', '🎶'] ['✅', '✅'] ['🐨', '🐨', '🐨', '🐨', '🐨', '🐨'] ['🕷'] ['☝'] ['☝'] ['☝'] ['☝'] ['☝'] ['⛰', '🐱', '⛰'] ['🐀', '🐀'] ['🗳'] ['👇'] ['🗳', '🗳', '🗳'] ['📌', '📌', '📌', '📌', '📌', '📌'] ['🐮'] ['🐶'] ['☕'] ['✅', '✅', '✅', '❓'] ['🗳', '🗳', '🗳'] ['🎶', '🎶'] ['💜'] ['💜']\n",
      "\n",
      "foxnews s tweets included the following emojis :\n",
      "\n",
      "\n",
      "nbcnews s tweets included the following emojis :\n",
      " ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷'] ['📷']\n",
      "\n",
      "mailonline s tweets included the following emojis :\n",
      " ['⚡'] ['⚡'] ['⚡'] ['⚡']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Apply `call_emoji_free` which calls the function to remove all emoji's\n",
    "# Apply 'url_free_text'which calls the function to remove all links\n",
    "\n",
    "pd.options.mode.chained_assignment = None  # default='warn' turn off one the warning ... \n",
    "\n",
    "for orga in df_import.agency.unique():\n",
    "    temp_df = df_import[df_import.agency ==orga]\n",
    "    print(orga,\"s tweets included the following emojis :\")\n",
    "    temp_df.text = temp_df.text.apply(call_emoji_free)\n",
    "    print(\"\\n\")\n",
    "    temp_df.text =temp_df.text.apply(url_free_text) \n",
    "    temp_df.text =temp_df.text.apply(url_free_text_s)\n",
    "    df_import[df_import.agency ==orga] = temp_df\n",
    "    \n",
    "df_prep=df_import # df_prep = result of preprocessing steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "\n",
    "\n",
    "#pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.0.0/en_core_web_md-2.0.0.tar.gz\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:45:10.524113Z",
     "start_time": "2020-12-23T07:45:09.835638Z"
    }
   },
   "outputs": [],
   "source": [
    "# Load spacy\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "#nlp = spacy.load('en_core_web_sm')\n",
    "nlp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:45:10.537408Z",
     "start_time": "2020-12-23T07:45:10.526625Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "# Custom stopwords\n",
    "custom_stopwords = ['hi','\\n','\\n\\n', '&amp;', ' ', '.', '-', 'got', \n",
    "                    \"it's\", 'it’s', \"i'm\", 'i’m', 'im', 'want', 'like', '$', '@']\n",
    "\n",
    "\n",
    "# Customize stop words by adding to the default list\n",
    "STOP_WORDS = nlp.Defaults.stop_words.union(custom_stopwords)\n",
    "\n",
    "# ALL_STOP_WORDS = spacy + gensim + wordcloud \n",
    "ALL_STOP_WORDS = STOP_WORDS.union(SW).union(stopwords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:45:10.545915Z",
     "start_time": "2020-12-23T07:45:10.541687Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "These are 10 of our stopwords:\n",
      "\n",
      "won't\n",
      "am\n",
      "seem\n",
      "let's\n",
      "myself\n",
      "show\n",
      "else\n",
      "will\n",
      "always\n",
      "it's\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "print(\"These are\",n,\"of our stopwords:\\n\")\n",
    "for i in list(ALL_STOP_WORDS)[:n]:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:45:14.635860Z",
     "start_time": "2020-12-23T07:45:10.549599Z"
    }
   },
   "outputs": [],
   "source": [
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(df_prep.text,batch_size=500):\n",
    "    doc_tokens = []    \n",
    "    for token in doc: \n",
    "        if token.text.lower() not in ALL_STOP_WORDS:   #in the code that I was following, they only had stop_words .... \n",
    "            doc_tokens.append(token.text.lower())   \n",
    "    tokens.append(doc_tokens)\n",
    "    \n",
    "# Makes tokens column\n",
    "df_prep['tokens'] = tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:45:14.653862Z",
     "start_time": "2020-12-23T07:45:14.638095Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1321057598104285185</th>\n",
       "      <td>huffpost</td>\n",
       "      <td>2020-10-27 11:54:10</td>\n",
       "      <td>The study showed just 4.4% of people in England had antibodies at the end of September.</td>\n",
       "      <td>2020-10-27 00:00:00</td>\n",
       "      <td>[study, showed, 4.4%, people, england, antibodies, end, september.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1337091887728107528</th>\n",
       "      <td>nbcnews</td>\n",
       "      <td>2020-12-10 17:48:42</td>\n",
       "      <td>Two members of a demolition crew remained missing Thursday following the collapse of a shuttered power plant in southern Ohio, authorities say.</td>\n",
       "      <td>2020-12-10 00:00:00</td>\n",
       "      <td>[members, demolition, crew, remained, missing, thursday, following, collapse, shuttered, power, plant, southern, ohio,, authorities, say.]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       agency          created_at  \\\n",
       "1321057598104285185  huffpost 2020-10-27 11:54:10   \n",
       "1337091887728107528  nbcnews  2020-12-10 17:48:42   \n",
       "\n",
       "                                                                                                                                                                 text  \\\n",
       "1321057598104285185  The study showed just 4.4% of people in England had antibodies at the end of September.                                                            \n",
       "1337091887728107528  Two members of a demolition crew remained missing Thursday following the collapse of a shuttered power plant in southern Ohio, authorities say.    \n",
       "\n",
       "                                    date  \\\n",
       "1321057598104285185  2020-10-27 00:00:00   \n",
       "1337091887728107528  2020-12-10 00:00:00   \n",
       "\n",
       "                                                                                                                                                         tokens  \n",
       "1321057598104285185  [study, showed, 4.4%, people, england, antibodies, end, september.]                                                                         \n",
       "1337091887728107528  [members, demolition, crew, remained, missing, thursday, following, collapse, shuttered, power, plant, southern, ohio,, authorities, say.]  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep.sample(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lemmatization and Synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:45:14.718347Z",
     "start_time": "2020-12-23T07:45:14.655781Z"
    }
   },
   "outputs": [],
   "source": [
    "# more help: https://spacy.io/api/token\n",
    "df_prep['tokens_back_to_text'] = [' '.join(map(str, l)) for l in df_prep['tokens']]\n",
    "\n",
    "\n",
    "# to remove weird characters and valueless words (in sense of our analysis),\n",
    "# we want to remove words with len<=2\n",
    "# to keep interesting words with this length, we define the following list\n",
    "\n",
    "list_useful_2char_word = [\"uk\",\"us\",\"ad\"] \n",
    "\n",
    "\n",
    "def get_lemmas(text):\n",
    "    '''Used to lemmatize the processed tweets'''\n",
    "    lemmas = []\n",
    "    \n",
    "    doc = nlp(text)\n",
    "    for token in doc: \n",
    "        if ((token.is_stop == False) and (token.is_punct == False) and \n",
    "                (token.pos_!= 'PRON') and (token.is_digit== False) and \n",
    "                ((len(token)>2) or (token.text in list_useful_2char_word))): \n",
    "            lemmas.append(token.lemma_)\n",
    "    \n",
    "    return lemmas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:17.168672Z",
     "start_time": "2020-12-23T07:45:14.720317Z"
    }
   },
   "outputs": [],
   "source": [
    "# important this cell might take some minutes to run\n",
    "# appllication of get_lemmas\n",
    "df_prep['lemmas'] = df_prep['tokens_back_to_text'].apply(get_lemmas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:17.193068Z",
     "start_time": "2020-12-23T07:48:17.170925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_back_to_text</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1339542616090800129</th>\n",
       "      <td>cnn</td>\n",
       "      <td>2020-12-17 12:07:01</td>\n",
       "      <td>Here's a look at what we know about Moderna's coronavirus vaccine candidate and how it compares to the Pfizer-BioNTech vaccine that was authorized and shipped out to the first Americans earlier this week</td>\n",
       "      <td>2020-12-17 00:00:00</td>\n",
       "      <td>[look, know, moderna's, coronavirus, vaccine, candidate, compares, pfizer-biontech, vaccine, authorized, shipped, americans, earlier, week]</td>\n",
       "      <td>look know moderna's coronavirus vaccine candidate compares pfizer-biontech vaccine authorized shipped americans earlier week</td>\n",
       "      <td>[look, know, moderna, coronavirus, vaccine, candidate, compare, pfizer, biontech, vaccine, authorize, ship, americans, early, week]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334161913777893379</th>\n",
       "      <td>huffpost</td>\n",
       "      <td>2020-12-02 15:46:02</td>\n",
       "      <td>How to know when your cloth mask is no longer protecting you and others from the coronavirus.</td>\n",
       "      <td>2020-12-02 00:00:00</td>\n",
       "      <td>[know, cloth, mask, longer, protecting, coronavirus.]</td>\n",
       "      <td>know cloth mask longer protecting coronavirus.</td>\n",
       "      <td>[know, cloth, mask, long, protect, coronavirus]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1339952328795938819</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-18 15:15:04</td>\n",
       "      <td>BREAKING: Supreme Court rules challenge to Trump census plan is premature</td>\n",
       "      <td>2020-12-18 00:00:00</td>\n",
       "      <td>[breaking:, supreme, court, rules, challenge, trump, census, plan, premature]</td>\n",
       "      <td>breaking: supreme court rules challenge trump census plan premature</td>\n",
       "      <td>[breaking, supreme, court, rule, challenge, trump, census, plan, premature]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1338596115692867586</th>\n",
       "      <td>foxnews</td>\n",
       "      <td>2020-12-14 21:25:58</td>\n",
       "      <td>Outside groups converge on Georgia with bus tours, on-the-ground volunteers with Senate on the line</td>\n",
       "      <td>2020-12-14 00:00:00</td>\n",
       "      <td>[outside, groups, converge, georgia, bus, tours,, on-the-ground, volunteers, senate, line]</td>\n",
       "      <td>outside groups converge georgia bus tours, on-the-ground volunteers senate line</td>\n",
       "      <td>[outside, group, converge, georgia, bus, tour, ground, volunteer, senate, line]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        agency          created_at  \\\n",
       "1339542616090800129  cnn       2020-12-17 12:07:01   \n",
       "1334161913777893379  huffpost  2020-12-02 15:46:02   \n",
       "1339952328795938819  yahoonews 2020-12-18 15:15:04   \n",
       "1338596115692867586  foxnews   2020-12-14 21:25:58   \n",
       "\n",
       "                                                                                                                                                                                                                             text  \\\n",
       "1339542616090800129  Here's a look at what we know about Moderna's coronavirus vaccine candidate and how it compares to the Pfizer-BioNTech vaccine that was authorized and shipped out to the first Americans earlier this week    \n",
       "1334161913777893379  How to know when your cloth mask is no longer protecting you and others from the coronavirus.                                                                                                                  \n",
       "1339952328795938819  BREAKING: Supreme Court rules challenge to Trump census plan is premature                                                                                                                                      \n",
       "1338596115692867586  Outside groups converge on Georgia with bus tours, on-the-ground volunteers with Senate on the line                                                                                                            \n",
       "\n",
       "                                    date  \\\n",
       "1339542616090800129  2020-12-17 00:00:00   \n",
       "1334161913777893379  2020-12-02 00:00:00   \n",
       "1339952328795938819  2020-12-18 00:00:00   \n",
       "1338596115692867586  2020-12-14 00:00:00   \n",
       "\n",
       "                                                                                                                                                          tokens  \\\n",
       "1339542616090800129  [look, know, moderna's, coronavirus, vaccine, candidate, compares, pfizer-biontech, vaccine, authorized, shipped, americans, earlier, week]   \n",
       "1334161913777893379  [know, cloth, mask, longer, protecting, coronavirus.]                                                                                         \n",
       "1339952328795938819  [breaking:, supreme, court, rules, challenge, trump, census, plan, premature]                                                                 \n",
       "1338596115692867586  [outside, groups, converge, georgia, bus, tours,, on-the-ground, volunteers, senate, line]                                                    \n",
       "\n",
       "                                                                                                                              tokens_back_to_text  \\\n",
       "1339542616090800129  look know moderna's coronavirus vaccine candidate compares pfizer-biontech vaccine authorized shipped americans earlier week   \n",
       "1334161913777893379  know cloth mask longer protecting coronavirus.                                                                                 \n",
       "1339952328795938819  breaking: supreme court rules challenge trump census plan premature                                                            \n",
       "1338596115692867586  outside groups converge georgia bus tours, on-the-ground volunteers senate line                                                \n",
       "\n",
       "                                                                                                                                                  lemmas  \n",
       "1339542616090800129  [look, know, moderna, coronavirus, vaccine, candidate, compare, pfizer, biontech, vaccine, authorize, ship, americans, early, week]  \n",
       "1334161913777893379  [know, cloth, mask, long, protect, coronavirus]                                                                                      \n",
       "1339952328795938819  [breaking, supreme, court, rule, challenge, trump, census, plan, premature]                                                          \n",
       "1338596115692867586  [outside, group, converge, georgia, bus, tour, ground, volunteer, senate, line]                                                      "
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep.sample(4) # visual inspection of lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:17.271522Z",
     "start_time": "2020-12-23T07:48:17.195246Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still 474 tweets with lemmas of length <=2 that are not allowed. \n",
      "These too short lemmas are:\n",
      "\n",
      "go,go,go,go,go,go,go,go,go,go,go,go,go,90,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,gp,go,go,up,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,up,go,go,go,go,go,go,go,go,go,go,go,go,go,40,go,go,80,go,go,go,up,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,60,80,go,go,go,go,go,re,go,re,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,up,go,go,go,go,go,go,re,up,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,re,go,go,go,go,go,f,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,tv,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,up,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,70,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,80,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,50,60,go,go,go,70,80,go,go,go,go,go,go,70,80,go,go,go,go,go,go,70,80,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,90,up,go,go,go,go,re,50,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,in,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,go,up,go,go,go,go,go,go,50,go,go,go,go,go,70,80,go,go,go,go,go,go,go,go,go,go,go,[None None None ... None None None]\n"
     ]
    }
   ],
   "source": [
    "## check if we still got tokens with length <= 2 that are not specified in out list\n",
    "def too_short_lemmas(x):\n",
    "    for single_lemma in x:\n",
    "        if (len(single_lemma)<=2) and (single_lemma not in list_useful_2char_word):\n",
    "                print(single_lemma,end=\",\")\n",
    "                \n",
    "\n",
    "def check_for_min_length_lemma(x):\n",
    "    too_short=False\n",
    "    for single_lemma in x:\n",
    "        if (len(single_lemma)<=2) and (single_lemma not in list_useful_2char_word):\n",
    "            too_short = True\n",
    "    return too_short\n",
    "\n",
    "\n",
    "print(\"There are still\",\n",
    "      len(df_prep[df_prep.lemmas.apply(lambda x:check_for_min_length_lemma(x))]),\n",
    "         \"tweets with lemmas of length <=2 that are not allowed.\",\n",
    "     \"\\nThese too short lemmas are:\\n\")\n",
    "\n",
    "# check if these lemmas can be removed or if they include important information\n",
    "print(df_prep.lemmas.apply(lambda x:too_short_lemmas(x)).values) \n",
    "\n",
    "\n",
    "# this happened because the token was for example going --> len(going)>2 but lemma = go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:17.335994Z",
     "start_time": "2020-12-23T07:48:17.273698Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are still 7 tweets with lemmas of length <=2 that are not allowed.\n"
     ]
    }
   ],
   "source": [
    "def remove_too_short(x):\n",
    "    for single_lemma in x:\n",
    "        if (len(single_lemma)<=2) and (single_lemma not in list_useful_2char_word):\n",
    "                x.remove(single_lemma)\n",
    "    return x\n",
    "\n",
    "df_prep.lemmas = df_prep.lemmas.apply(lambda x:remove_too_short(x))\n",
    "\n",
    "print(\"There are still\",\n",
    "      len(df_prep[df_prep.lemmas.apply(lambda x:check_for_min_length_lemma(x))]),\n",
    "         \"tweets with lemmas of length <=2 that are not allowed.\")\n",
    "# because of the small amount the effort to catch these slips was rated to high"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:17.576914Z",
     "start_time": "2020-12-23T07:48:17.338023Z"
    }
   },
   "outputs": [],
   "source": [
    "# Make lemmas a string again\n",
    "df_prep['lemmas_back_to_text'] = [' '.join(map(str,l)) for l in df_prep['lemmas']]\n",
    "\n",
    "\n",
    "# handling of synonyms for topic based features\n",
    "trump_synonyms = [\"donald\", \"donald trump\"] \n",
    "biden_synonyms = [\"joe\",\"joe biden\"]\n",
    "harris_synonyms = [\"kamala\",\"kamala harris\"]\n",
    "covid_synonyms = [\"corona\",\"covid-19\",\"corona virus\",\"virus\",\"coronavirus\"]\n",
    "\n",
    "def replace_syn(repl_string,synonyms,col):\n",
    "    for syn in synonyms:\n",
    "        col_output = col.str.replace(syn,repl_string)\n",
    "    return col_output\n",
    "        \n",
    "df_prep['lemmas_back_to_text'] = replace_syn(\"trump\",trump_synonyms,df_prep['lemmas_back_to_text'])\n",
    "df_prep['lemmas_back_to_text'] = replace_syn(\"biden\",biden_synonyms,df_prep['lemmas_back_to_text'])\n",
    "df_prep['lemmas_back_to_text'] = replace_syn(\"harris\",harris_synonyms,df_prep['lemmas_back_to_text'])\n",
    "df_prep['lemmas_back_to_text'] = replace_syn(\"covid\",covid_synonyms,df_prep['lemmas_back_to_text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:17.602178Z",
     "start_time": "2020-12-23T07:48:17.579310Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_back_to_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lemmas_back_to_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1316101437886279687</th>\n",
       "      <td>nytimes</td>\n",
       "      <td>2020-10-13 19:40:09</td>\n",
       "      <td>He immigrated to the U.S. after his home country banned same-sex marriage. He later founded a homeless shelter for those seeking asylum. This new award is giving him $200,000 to help finance his endeavor.</td>\n",
       "      <td>2020-10-13 00:00:00</td>\n",
       "      <td>[immigrated, u.s., home, country, banned, same-sex, marriage., later, founded, homeless, shelter, seeking, asylum., new, award, giving, $200,000, help, finance, endeavor.]</td>\n",
       "      <td>immigrated u.s. home country banned same-sex marriage. later founded homeless shelter seeking asylum. new award giving $200,000 help finance endeavor.</td>\n",
       "      <td>[immigrated, u.s, home, country, ban, sex, marriage, later, found, homeless, shelter, seek, asylum, new, award, give, 200,000, help, finance, endeavor]</td>\n",
       "      <td>immigrated u.s home country ban sex marriage later found homeless shelter seek asylum new award give 200,000 help finance endeavor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1334903273162203137</th>\n",
       "      <td>foxnews</td>\n",
       "      <td>2020-12-04 16:51:56</td>\n",
       "      <td>Moderna reveals how long coronavirus immunity will last with their vaccine</td>\n",
       "      <td>2020-12-04 00:00:00</td>\n",
       "      <td>[moderna, reveals, long, coronavirus, immunity, vaccine]</td>\n",
       "      <td>moderna reveals long coronavirus immunity vaccine</td>\n",
       "      <td>[moderna, reveal, long, coronavirus, immunity, vaccine]</td>\n",
       "      <td>moderna reveal long covid immunity vaccine</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321897361556361216</th>\n",
       "      <td>cnn</td>\n",
       "      <td>2020-10-29 19:31:05</td>\n",
       "      <td>After a string of dismal earnings and missed expectations last year, Tupperware is reporting soaring profits. The company announced it made $477.2 million in sales in the third quarter of 2020 — up 14% versus last year.</td>\n",
       "      <td>2020-10-29 00:00:00</td>\n",
       "      <td>[string, dismal, earnings, missed, expectations, year,, tupperware, reporting, soaring, profits., company, announced, $477.2, million, sales, quarter, 2020, —, 14%, versus, year.]</td>\n",
       "      <td>string dismal earnings missed expectations year, tupperware reporting soaring profits. company announced $477.2 million sales quarter 2020 — 14% versus year.</td>\n",
       "      <td>[stre, dismal, earning, miss, expectation, year, tupperware, report, soar, profit, company, announce, 477.2, million, sale, quarter, versus, year]</td>\n",
       "      <td>stre dismal earning miss expectation year tupperware report soar profit company announce 477.2 million sale quarter versus year</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1321267547958902784</th>\n",
       "      <td>nbcnews</td>\n",
       "      <td>2020-10-28 01:48:26</td>\n",
       "      <td>The Texas Supreme Court has upheld Gov. Abbott's order limiting counties to 1 drop-off site for absentee ballots, dealing a blow to Democrats and voting rights groups that won a temporary injunction blocking the mandate.</td>\n",
       "      <td>2020-10-28 00:00:00</td>\n",
       "      <td>[texas, supreme, court, upheld, gov., abbott's, order, limiting, counties, 1, drop-off, site, absentee, ballots,, dealing, blow, democrats, voting, rights, groups, won, temporary, injunction, blocking, mandate.]</td>\n",
       "      <td>texas supreme court upheld gov. abbott's order limiting counties 1 drop-off site absentee ballots, dealing blow democrats voting rights groups won temporary injunction blocking mandate.</td>\n",
       "      <td>[texas, supreme, court, uphold, gov, abbott, order, limit, county, drop, site, absentee, ballot, deal, blow, democrats, vote, right, group, win, temporary, injunction, block, mandate]</td>\n",
       "      <td>texas supreme court uphold gov abbott order limit county drop site absentee ballot deal blow democrats vote right group win temporary injunction block mandate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1318503496296038400</th>\n",
       "      <td>cnn</td>\n",
       "      <td>2020-10-20 10:45:04</td>\n",
       "      <td>QAnon may be a way for supporters to distract themselves from the failures of a President they see as the hero of a fight against villainy | Analysis by @donie</td>\n",
       "      <td>2020-10-20 00:00:00</td>\n",
       "      <td>[qanon, way, supporters, distract, failures, president, hero, fight, villainy, |, analysis, @donie]</td>\n",
       "      <td>qanon way supporters distract failures president hero fight villainy | analysis @donie</td>\n",
       "      <td>[qanon, way, supporter, distract, failure, president, hero, fight, villainy, analysis, @donie]</td>\n",
       "      <td>qanon way supporter distract failure president hero fight villainy analysis @donie</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      agency          created_at  \\\n",
       "1316101437886279687  nytimes 2020-10-13 19:40:09   \n",
       "1334903273162203137  foxnews 2020-12-04 16:51:56   \n",
       "1321897361556361216  cnn     2020-10-29 19:31:05   \n",
       "1321267547958902784  nbcnews 2020-10-28 01:48:26   \n",
       "1318503496296038400  cnn     2020-10-20 10:45:04   \n",
       "\n",
       "                                                                                                                                                                                                                                              text  \\\n",
       "1316101437886279687  He immigrated to the U.S. after his home country banned same-sex marriage. He later founded a homeless shelter for those seeking asylum. This new award is giving him $200,000 to help finance his endeavor.                    \n",
       "1334903273162203137  Moderna reveals how long coronavirus immunity will last with their vaccine                                                                                                                                                      \n",
       "1321897361556361216  After a string of dismal earnings and missed expectations last year, Tupperware is reporting soaring profits. The company announced it made $477.2 million in sales in the third quarter of 2020 — up 14% versus last year.     \n",
       "1321267547958902784  The Texas Supreme Court has upheld Gov. Abbott's order limiting counties to 1 drop-off site for absentee ballots, dealing a blow to Democrats and voting rights groups that won a temporary injunction blocking the mandate.    \n",
       "1318503496296038400  QAnon may be a way for supporters to distract themselves from the failures of a President they see as the hero of a fight against villainy | Analysis by @donie                                                                 \n",
       "\n",
       "                                    date  \\\n",
       "1316101437886279687  2020-10-13 00:00:00   \n",
       "1334903273162203137  2020-12-04 00:00:00   \n",
       "1321897361556361216  2020-10-29 00:00:00   \n",
       "1321267547958902784  2020-10-28 00:00:00   \n",
       "1318503496296038400  2020-10-20 00:00:00   \n",
       "\n",
       "                                                                                                                                                                                                                                  tokens  \\\n",
       "1316101437886279687  [immigrated, u.s., home, country, banned, same-sex, marriage., later, founded, homeless, shelter, seeking, asylum., new, award, giving, $200,000, help, finance, endeavor.]                                           \n",
       "1334903273162203137  [moderna, reveals, long, coronavirus, immunity, vaccine]                                                                                                                                                              \n",
       "1321897361556361216  [string, dismal, earnings, missed, expectations, year,, tupperware, reporting, soaring, profits., company, announced, $477.2, million, sales, quarter, 2020, —, 14%, versus, year.]                                   \n",
       "1321267547958902784  [texas, supreme, court, upheld, gov., abbott's, order, limiting, counties, 1, drop-off, site, absentee, ballots,, dealing, blow, democrats, voting, rights, groups, won, temporary, injunction, blocking, mandate.]   \n",
       "1318503496296038400  [qanon, way, supporters, distract, failures, president, hero, fight, villainy, |, analysis, @donie]                                                                                                                   \n",
       "\n",
       "                                                                                                                                                                                           tokens_back_to_text  \\\n",
       "1316101437886279687  immigrated u.s. home country banned same-sex marriage. later founded homeless shelter seeking asylum. new award giving $200,000 help finance endeavor.                                      \n",
       "1334903273162203137  moderna reveals long coronavirus immunity vaccine                                                                                                                                           \n",
       "1321897361556361216  string dismal earnings missed expectations year, tupperware reporting soaring profits. company announced $477.2 million sales quarter 2020 — 14% versus year.                               \n",
       "1321267547958902784  texas supreme court upheld gov. abbott's order limiting counties 1 drop-off site absentee ballots, dealing blow democrats voting rights groups won temporary injunction blocking mandate.   \n",
       "1318503496296038400  qanon way supporters distract failures president hero fight villainy | analysis @donie                                                                                                      \n",
       "\n",
       "                                                                                                                                                                                                      lemmas  \\\n",
       "1316101437886279687  [immigrated, u.s, home, country, ban, sex, marriage, later, found, homeless, shelter, seek, asylum, new, award, give, 200,000, help, finance, endeavor]                                   \n",
       "1334903273162203137  [moderna, reveal, long, coronavirus, immunity, vaccine]                                                                                                                                   \n",
       "1321897361556361216  [stre, dismal, earning, miss, expectation, year, tupperware, report, soar, profit, company, announce, 477.2, million, sale, quarter, versus, year]                                        \n",
       "1321267547958902784  [texas, supreme, court, uphold, gov, abbott, order, limit, county, drop, site, absentee, ballot, deal, blow, democrats, vote, right, group, win, temporary, injunction, block, mandate]   \n",
       "1318503496296038400  [qanon, way, supporter, distract, failure, president, hero, fight, villainy, analysis, @donie]                                                                                            \n",
       "\n",
       "                                                                                                                                                                lemmas_back_to_text  \n",
       "1316101437886279687  immigrated u.s home country ban sex marriage later found homeless shelter seek asylum new award give 200,000 help finance endeavor                              \n",
       "1334903273162203137  moderna reveal long covid immunity vaccine                                                                                                                      \n",
       "1321897361556361216  stre dismal earning miss expectation year tupperware report soar profit company announce 477.2 million sale quarter versus year                                 \n",
       "1321267547958902784  texas supreme court uphold gov abbott order limit county drop site absentee ballot deal blow democrats vote right group win temporary injunction block mandate  \n",
       "1318503496296038400  qanon way supporter distract failure president hero fight villainy analysis @donie                                                                              "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:17.610419Z",
     "start_time": "2020-12-23T07:48:17.604301Z"
    }
   },
   "outputs": [],
   "source": [
    "lemma_tokens_all = []\n",
    "# Tokenizer function\n",
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Parses a string into a list of semantic units (words)\n",
    "    Args:\n",
    "        text (str): The string that the function will tokenize.\n",
    "    Returns:\n",
    "        list: tokens parsed out\n",
    "    \"\"\"\n",
    "    # first of the following does not seem to work - or is this bcs of string digits\n",
    "    tokens = re.sub(r'[^a-zA-Z 0-9]', '', text) \n",
    "    tokens = re.sub(r'[%s]' % re.escape(string.punctuation), '', text) # Remove punctuation\n",
    "    tokens = re.sub(r'\\w*\\d\\w*', '', text) # Remove words containing numbers\n",
    "    tokens = re.sub(r'@*!*\\$*', '', text) # Remove @ ! $\n",
    "    tokens = tokens.strip(',') \n",
    "    tokens = tokens.strip('?') \n",
    "    tokens = tokens.strip('!') \n",
    "    tokens = tokens.strip(\"'\") \n",
    "    tokens = tokens.strip(\".\") \n",
    "\n",
    "\n",
    "    tokens = tokens.lower().split() # Make text lowercase and split it\n",
    "                \n",
    "    lemma_tokens_all.extend(tokens)\n",
    "    return tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:18.839247Z",
     "start_time": "2020-12-23T07:48:17.612466Z"
    }
   },
   "outputs": [],
   "source": [
    "# Apply tokenizer to df_all_orgas_cut_off and parallel create a list for every agency\n",
    "# witl all lemma tokens\n",
    "\n",
    "temp_series = pd.Series()\n",
    "for i in df_prep.agency.unique():\n",
    "    temp_df_part= df_prep[df_prep.agency==i]\n",
    "    exec(f'list_{i}=[]') # create list to store all agency specific lemma tokens\n",
    "    temp_df_part['lemma_tokens'] = temp_df_part['lemmas_back_to_text'].apply(tokenize)\n",
    "    exec(f'list_{i}.extend(lemma_tokens_all)')\n",
    "    lemma_tokens_all=[]\n",
    "    temp_series= temp_series.append(temp_df_part['lemma_tokens'])\n",
    "\n",
    "df_prep['lemma_tokens'] =temp_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:18.857257Z",
     "start_time": "2020-12-23T07:48:18.841859Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_back_to_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lemmas_back_to_text</th>\n",
       "      <th>lemma_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1340367556947943425</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-19 18:45:03</td>\n",
       "      <td>A $20,000 fee to change a wedding date? New bill aims to protect people from 'bad actor catering halls'</td>\n",
       "      <td>2020-12-19 00:00:00</td>\n",
       "      <td>[$20,000, fee, change, wedding, date?, new, aims, protect, people, 'bad, actor, catering, halls']</td>\n",
       "      <td>$20,000 fee change wedding date? new aims protect people 'bad actor catering halls'</td>\n",
       "      <td>[20,000, fee, change, wedding, date, new, aim, protect, people, bad, actor, catering, hall]</td>\n",
       "      <td>20,000 fee change wedding date new aim protect people bad actor catering hall</td>\n",
       "      <td>[20,000, fee, change, wedding, date, new, aim, protect, people, bad, actor, catering, hall]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340329823282016256</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-19 16:15:06</td>\n",
       "      <td>U.S. intelligence agencies are increasingly focused on domestic extremists. Their latest target: Satanists</td>\n",
       "      <td>2020-12-19 00:00:00</td>\n",
       "      <td>[u.s., intelligence, agencies, increasingly, focused, domestic, extremists., latest, target:, satanists]</td>\n",
       "      <td>u.s. intelligence agencies increasingly focused domestic extremists. latest target: satanists</td>\n",
       "      <td>[u.s, intelligence, agency, increasingly, focus, domestic, extremist, late, target, satanist]</td>\n",
       "      <td>u.s intelligence agency increasingly focus domestic extremist late target satanist</td>\n",
       "      <td>[u.s, intelligence, agency, increasingly, focus, domestic, extremist, late, target, satanist]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        agency          created_at  \\\n",
       "1340367556947943425  yahoonews 2020-12-19 18:45:03   \n",
       "1340329823282016256  yahoonews 2020-12-19 16:15:06   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "1340367556947943425  A $20,000 fee to change a wedding date? New bill aims to protect people from 'bad actor catering halls'        \n",
       "1340329823282016256  U.S. intelligence agencies are increasingly focused on domestic extremists. Their latest target: Satanists     \n",
       "\n",
       "                                    date  \\\n",
       "1340367556947943425  2020-12-19 00:00:00   \n",
       "1340329823282016256  2020-12-19 00:00:00   \n",
       "\n",
       "                                                                                                                       tokens  \\\n",
       "1340367556947943425  [$20,000, fee, change, wedding, date?, new, aims, protect, people, 'bad, actor, catering, halls']          \n",
       "1340329823282016256  [u.s., intelligence, agencies, increasingly, focused, domestic, extremists., latest, target:, satanists]   \n",
       "\n",
       "                                                                                               tokens_back_to_text  \\\n",
       "1340367556947943425  $20,000 fee change wedding date? new aims protect people 'bad actor catering halls'             \n",
       "1340329823282016256  u.s. intelligence agencies increasingly focused domestic extremists. latest target: satanists   \n",
       "\n",
       "                                                                                                            lemmas  \\\n",
       "1340367556947943425  [20,000, fee, change, wedding, date, new, aim, protect, people, bad, actor, catering, hall]     \n",
       "1340329823282016256  [u.s, intelligence, agency, increasingly, focus, domestic, extremist, late, target, satanist]   \n",
       "\n",
       "                                                                                    lemmas_back_to_text  \\\n",
       "1340367556947943425  20,000 fee change wedding date new aim protect people bad actor catering hall        \n",
       "1340329823282016256  u.s intelligence agency increasingly focus domestic extremist late target satanist   \n",
       "\n",
       "                                                                                                      lemma_tokens  \n",
       "1340367556947943425  [20,000, fee, change, wedding, date, new, aim, protect, people, bad, actor, catering, hall]    \n",
       "1340329823282016256  [u.s, intelligence, agency, increasingly, focus, domestic, extremist, late, target, satanist]  "
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_prep.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:48:18.865283Z",
     "start_time": "2020-12-23T07:48:18.860065Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 40 lemma tokens of yahoonews:\n",
      "20,000 fee change wedding date new aim protect people bad actor catering hall u.s intelligence agency increasingly focus domestic extremist late target satanist rebuild trust justice department start end biden choice attorney general biden say lindsey graham personal disappointment recognize\n"
     ]
    }
   ],
   "source": [
    "# inspect some lemma tokens for one of the agencies\n",
    "n=40\n",
    "print(\"first\",n,\"lemma tokens of yahoonews:\")\n",
    "print(*[i for i in list_yahoonews[:n]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:49:46.837192Z",
     "start_time": "2020-12-23T07:49:46.759196Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ue is of type <class 'str'> and found in: ['story', 'futureproof', 'project', 'world', 'ready', 'crisis', 'series', 'collaboration', 'huffpost', 'ue', 'greenpeace', 'uk', 'journalism', 'project'] \n",
      "\n",
      "ue is of type <class 'str'> and found in: ['story', 'futureproof', 'project', 'world', 'ready', 'crisis', 'series', 'collaboration', 'huffpost', 'ue', 'greenpeace', 'uk', 'journalism', 'project'] \n",
      "\n",
      "1930 is of type <class 'str'> and found in: ['idea', 'gdp', 'value', 'good', 'service', 'exchange', 'country', 'come', '1930', 'period', 'characterize', 'soar', 'unemployment', 'deep', 'inequality', 'help', 'measure', 'country', 'recovery', 'great', 'depression'] \n",
      "\n",
      "ue is of type <class 'str'> and found in: ['story', 'futureproof', 'project', 'world', 'ready', 'crisis', 'series', 'collaboration', 'huffpost', 'ue', 'greenpeace', 'uk', 'journalism', 'project'] \n",
      "\n",
      "1960 is of type <class 'str'> and found in: ['fast', 'forward', 'century', 'u.s', '1960', 'basic', 'income', 'gathering', 'steam', 'martin', 'luth', 'king', 'raise', 'idea', 'book', 'chaos', 'community'] \n",
      "\n",
      "1950 is of type <class 'str'> and found in: ['silence', 'congressional', 'republican', 'leader', 'trump', 'unfounded', 'claim', 'election', 'fraud', 'grow', 'wild', 'venomous', 'increasingly', 'resemble', 'party', 'deference', 'gop', 'sen', 'joe', 'mccarthy', 'early', '1950', 'ronald', 'brownstein', 'write', 'analysis'] \n",
      "\n",
      "1950 is of type <class 'str'> and found in: ['jean', 'graetz', 'white', 'people', 'montgomery', 'ala', 'participate', 'city', 'civil', 'right', 'movement', '1950', 'push', 'forward', 'faced', 'slash', 'tire', 'obscene', 'phone', 'call', 'multiple', 'bombing', 'die'] \n",
      "\n",
      "1800 is of type <class 'str'> and found in: ['able', 'answer', 'kind', 'question', 'new', 'u.s', 'citizenship', 'test', 'leader', 'woman', 'right', 'movement', '1800'] \n",
      "\n",
      "2000 is of type <class 'str'> and found in: ['nearly', '100,000', 'people', 'file', 'sex', 'abuse', 'claim', 'boy', 'scouts', 'america', 'far', 'surpass', 'number', 'allegation', 'catholic', 'church', 'face', 'early', '2000', 'hear', 'man', 'story', 'today', 'episode', 'daily'] \n",
      "\n",
      "80 is of type <class 'str'> and found in: ['lovesick', 'song', 'silly', 'game', 'indelible', 'high', 'note', 'play', 'pivotal', 'lover', 'rock', 'director', 'steve', 'mcqueen', 'new', 'anthology', 'examine', 'black', 'british', 'life', '80'] \n",
      "\n",
      "1980 is of type <class 'str'> and found in: ['war', 'drug', 'ramp', '1980', 'study', 'show', 'people', 'color', 'disproportionately', 'target', 'marijuana', 'enforcement', 'conviction', 'prevent', 'people', 'find', 'job', 'get', 'public', 'housing', 'maintain', 'custody', 'child', '5/10'] \n",
      "\n",
      "1960 is of type <class 'str'> and found in: ['biden', 'non', 'radical', '1960'] \n",
      "\n",
      "05 is of type <class 'str'> and found in: ['05', 'percent', 'healthy', 'under-70s', 'die', 'covid', 'study', 'claim'] \n",
      "\n",
      "05 is of type <class 'str'> and found in: ['05', 'percent', 'healthy', 'under-70s', 'die', 'covid', 'study', 'claim'] \n",
      "\n",
      "1990 is of type <class 'str'> and found in: ['federal', 'judge', 'rule', 'department', 'justice', 'defend', 'president', 'trump', 'defamation', 'suit', 'bring', 'jean', 'carroll', 'accuse', 'trump', 'raping', '1990'] \n",
      "\n",
      "1970 is of type <class 'str'> and found in: ['thing', 'trump', 'administration', 'say', 'black', 'americans', 'remark', 'damning', 'one', 'tap', 'cruel', 'welfare', 'queen', 'rhetoric', 'late', '1970', 'brandontensley', 'remark', 'jared', 'kushner', 'analysis'] \n",
      "\n",
      "60 is of type <class 'str'> and found in: ['president', 'try', 'win', 'suburban', 'women', 'series', 'bizarre', 'racist', 'appeal', 'target', 'stereotype', '60', 'american', 'woman', 'actually', 'live', 'area', 'today', 'maevereston', 'write', 'analysis'] \n",
      "\n",
      "80 is of type <class 'str'> and found in: ['nearly', 'year', 'coca', 'cola', 'discontinue', 'diet', 'soda', 'tab', 'acquire', 'huge', 'fan', 'base', '80', 'maintain', 'small', 'devote', 'customer', 'base', 'year'] \n",
      "\n",
      "1940 is of type <class 'str'> and found in: ['rhonda', 'fleming', 'film', 'star', '1940', '50s', 'know', 'queen', 'technicolor', 'die', 'accord', 'assistant'] \n",
      "\n",
      "80 is of type <class 'str'> and found in: ['nearly', 'year', 'coca', 'cola', 'discontinue', 'diet', 'soda', 'tab', 'acquire', 'huge', 'fan', 'base', '80', 'maintain', 'small', 'devote', 'customer', 'base', 'year'] \n",
      "\n",
      "80 is of type <class 'str'> and found in: ['nearly', 'year', 'coca', 'cola', 'discontinue', 'diet', 'soda', 'tab', 'acquire', 'huge', 'fan', 'base', '80', 'maintain', 'small', 'devote', 'customer', 'base', 'year'] \n",
      "\n",
      "go is of type <class 'str'> and found in: ['iowa', 'wisconsin', 'georgia', 'go', 'florida', 'maybe', 'place', 'biden', 'say', 'stop', 'voter', 'center', 'chester', 'pennsylvania'] \n",
      "\n",
      "1960 is of type <class 'str'> and found in: ['legal', 'segregation', 'end', '1960', 'intellectual', 'activist', 'try', 'world', 'law', 'change', 'remain', 'ineffably', 'prejudice', 'bias', 'intolerance', 'insufficient', 'white', 'supremacy', 'see', 'effective'] \n",
      "\n",
      "1960 is of type <class 'str'> and found in: ['biden', 'exposure', 'main', 'issue', '1960', 'civil', 'right', 'vietnam', 'war', 'come', 'campus', 'long', 'cite', 'experience', 'lifeguard', 'pool', 'largely', 'black', 'town', 'instrumental', 'understand', 'racial', 'divide'] \n",
      "\n",
      "1960 is of type <class 'str'> and found in: ['college', 'law', 'school', 'student', 'tumultuous', '1960', 'biden', 'unmove', 'fury', 'civil', 'right', 'vietnam', 'war', 'display', 'peer', 'people', 'march', 'biden', 'say', 'run', 'office'] \n",
      "\n",
      "1960 is of type <class 'str'> and found in: ['friend', 'classmate', 'know', 'biden', 'decade', 'man', 'keen', 'bring', '1950s', 'sensibility', '1960', 'nice', 'house', 'cul', 'sac', 'kind', 'guy', 'spend', 'weekend', '20-something', 'husband', 'scout', 'real', 'estate', 'corvette'] \n",
      "\n",
      "1940 is of type <class 'str'> and found in: ['rhonda', 'fleming', 'actress', 'know', 'hollywood', 'western', 'film', 'noir', 'adventure', 'movie', '1940', 'die', 'wednesday'] \n",
      "\n",
      "1980 is of type <class 'str'> and found in: ['debate', 'face', 'mask', 'pandemic', 'sound', 'lot', 'argument', 'seatbelt', '1980', 'look', 'fight', 'balance', 'individual', 'public', 'interest'] \n",
      "\n",
      "1980 is of type <class 'str'> and found in: ['friends', 'harris', 'popular', 'comfortable', 'skin', 'howard', 'hub', 'washington', 'black', 'political', 'elite', '1980', 'cute', 'free', 'independent', 'big', 'city', 'harris', 'classmates', 'say'] \n",
      "\n",
      "1960 is of type <class 'str'> and found in: ['cecilia', 'chiang', 'famed', 'restaurateur', 'help', 'introduce', 'authentic', 'chinese', 'food', 'america', '1960', 'die', 'todayshow'] \n",
      "\n",
      "2000 is of type <class 'str'> and found in: ['santana', 'regularly', 'vote', 'early', '2000', 'election', 'unique', 'experience', 'story', 'teen', 'rise', 'trump', 'political', 'figure', 'tie', 'beginning', '3/9'] \n",
      "\n",
      "1940 is of type <class 'str'> and found in: ['rhonda', 'fleming', 'star', '1940', 'dub', 'queen', 'technicolor', 'die', 'wednesday', 'age', 'accord', 'secretary'] \n",
      "\n",
      "co is of type <class 'str'> and found in: ['new', 'mexico', 'gov', 'michelle', 'lujan', 'grisham', 'new', 'mexico', 'break', 'record', 'daily', 'covid-19', 'case', 'day', 'row', 'today', 'update', 'new', 'case', 'additional', 'death', 'include', 'new', 'mexico', 'young', 'covid-19', 'death', 'date', 'girl', 'teen', 'eddy', 'co'] \n",
      "\n",
      "80 is of type <class 'str'> and found in: ['coca', 'cola', 'announce', 'stop', 'sell', 'can', 'tab', 'end', 'year', 'discontinue', 'popular', 'diet', 'soda', '80'] \n",
      "\n",
      "There are only 34 lemma tokens left, that do not comply to our length rule\n"
     ]
    }
   ],
   "source": [
    "# check how many tokens are left that do not comply to our length rule (if the number is small we keep them)\n",
    "counter_01=0\n",
    "for i in df_prep['lemma_tokens']:\n",
    "    for k in i: \n",
    "        if (k.isnumeric()) or (len(k)<3) and (k  not in list_useful_2char_word):\n",
    "            counter_01 +=1\n",
    "            a=k\n",
    "            print(k,\"is of type\",type(k),\"and found in:\",i,\"\\n\")\n",
    "print(\"There are only\",counter_01,\"lemma tokens left, that do not comply to our length rule\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-11-04T22:38:09.122095Z",
     "start_time": "2020-11-04T22:38:09.119341Z"
    }
   },
   "source": [
    "\n",
    "#  create id2word and pre-process further \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:50.620509Z",
     "start_time": "2020-12-23T07:57:50.093215Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create a gensim dictionary out of ALL lemma tokens\n",
    "gensim_dict = Dictionary(df_prep['lemma_tokens'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:50.633030Z",
     "start_time": "2020-12-23T07:57:50.623035Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dict was created by lookint at 20796 tweets \n",
      "It contains 19025 different lemma tokens \n",
      "\n",
      "Here are the first 60 tweets:\n",
      "20,000, actor, aim, bad, catering, change, date, fee, hall, new, people, protect, wedding, agency, domestic, extremist, focus, increasingly, intelligence, late, satanist, target, u.s, attorney, biden, choice, department, end, general, justice, rebuild, start, trust, disappointment, elect, graham, lindsey, personal, president, recognize, say, carolina, christmas, church, count, covid, musical, north, positive, test, 2-day, avoid, congress, covid-19, deal, elusive, remain, seek, shutdown, spending, "
     ]
    }
   ],
   "source": [
    "def some_stats_on_gen_dict(gd):\n",
    "    n=60\n",
    "    #This module implements the concept of a Dictionary \n",
    "    # – a mapping between words and their integer ids.\n",
    "    print(\"The dict was created by lookint at\",gd.num_docs,\n",
    "          \"tweets \\nIt contains\",len(gd),\n",
    "          \"different lemma tokens \\n\\nHere are the first\",n,\"tweets:\")\n",
    "    for i in range(n):\n",
    "        print (gd.get(i),end=\", \")\n",
    "\n",
    "some_stats_on_gen_dict(gensim_dict)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:51.087542Z",
     "start_time": "2020-12-23T07:57:50.636206Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "# create a corpus with all final lemma tokens for each agency\n",
    "lemma_tokens_all = []\n",
    "for sublist in df_prep.agency.unique():\n",
    "    exec(f'lemma_tokens_all.append(list_{sublist})')\n",
    "\n",
    "print(len(lemma_tokens_all)) \n",
    "\n",
    "corpus_all = [gensim_dict.doc2bow(d) for d in lemma_tokens_all]\n",
    "\n",
    "# create work frequency list for all agency \n",
    "word_frequencies = [[(gensim_dict[id], frequence) for id, \n",
    "                     frequence in couple] for couple in corpus_all]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The following code creates the feature set for the bag of words approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:51.108489Z",
     "start_time": "2020-12-23T07:57:51.092091Z"
    }
   },
   "outputs": [],
   "source": [
    "word_frequencies_sorted = []\n",
    "for tuples in word_frequencies:\n",
    "    word_frequencies_sorted.append(sorted(tuples, key=lambda tup: tup[1], reverse = True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:51.186204Z",
     "start_time": "2020-12-23T07:57:51.111746Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[37, 63, 57, 63, 39, 51, 69]\n"
     ]
    }
   ],
   "source": [
    "k = []\n",
    "for word_frequencies_agency in word_frequencies_sorted:\n",
    "    cumu_count = [0]\n",
    "    counter = 1\n",
    "    for i in word_frequencies_agency:\n",
    "        cumu_count.append(cumu_count[counter-1]+i[1])\n",
    "        counter+=1\n",
    "\n",
    "    num_element = 0\n",
    "    \n",
    "    for i in cumu_count:\n",
    "        if i > 0.2 * max(cumu_count):\n",
    "            k.append(num_element)\n",
    "            break\n",
    "        num_element += 1\n",
    "\n",
    "print(k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:51.193956Z",
     "start_time": "2020-12-23T07:57:51.188945Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words across all agencies: 379\n",
      "Number of unique words across all agencies: 158\n"
     ]
    }
   ],
   "source": [
    "agency = 0\n",
    "feature_list = []\n",
    "for j in k:\n",
    "    for i in range(0,j):\n",
    "        feature_list.append(word_frequencies_sorted[agency][i][0])\n",
    "    agency +=1\n",
    "    \n",
    "\n",
    "unique_feature_list = list(set(feature_list))\n",
    "print(\"Number of words across all agencies:\",len(feature_list))\n",
    "print(\"Number of unique words across all agencies:\",len(unique_feature_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:51.203539Z",
     "start_time": "2020-12-23T07:57:51.196748Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['positive',\n",
       " 'show',\n",
       " 'hunter',\n",
       " 'trump',\n",
       " 'russian',\n",
       " 'black',\n",
       " 'analysis',\n",
       " 'accord',\n",
       " 'biden',\n",
       " 'senate',\n",
       " 'change',\n",
       " 'say',\n",
       " 'democrats',\n",
       " 'national',\n",
       " 'woman',\n",
       " 'sweden',\n",
       " 'attack',\n",
       " 'texas',\n",
       " 'come',\n",
       " 'covid-19',\n",
       " 'right',\n",
       " 'debate',\n",
       " 'thousand',\n",
       " 'gop',\n",
       " 'covid',\n",
       " 'york',\n",
       " 'million',\n",
       " 'case',\n",
       " 'coney',\n",
       " 'chinese',\n",
       " 'warn',\n",
       " 'body',\n",
       " 'americans',\n",
       " 'month',\n",
       " 'democratic',\n",
       " 'family',\n",
       " 'mother',\n",
       " 'late',\n",
       " 'claim',\n",
       " 'test',\n",
       " 'supreme',\n",
       " 'actor',\n",
       " 'time',\n",
       " 'expert',\n",
       " 'france',\n",
       " 'girl',\n",
       " 'georgia',\n",
       " 'story',\n",
       " 'president',\n",
       " 'long',\n",
       " 'host',\n",
       " 'take',\n",
       " 'question',\n",
       " 'early',\n",
       " 'king',\n",
       " 'court',\n",
       " 'owner',\n",
       " 'official',\n",
       " 'death',\n",
       " 'face',\n",
       " 'campaign',\n",
       " 'record',\n",
       " 'man',\n",
       " 'human',\n",
       " 'life',\n",
       " 'virus',\n",
       " 'find',\n",
       " 'people',\n",
       " 'thursday',\n",
       " 'times',\n",
       " 'leave',\n",
       " 'china',\n",
       " 'live',\n",
       " 'nov',\n",
       " 'senator',\n",
       " 'study',\n",
       " 'look',\n",
       " 'russia',\n",
       " 'know',\n",
       " 'terrorist',\n",
       " 'macron',\n",
       " 'new',\n",
       " 'moment',\n",
       " 'baby',\n",
       " 'win',\n",
       " 'election',\n",
       " 'vaccine',\n",
       " 'kill',\n",
       " 'ballot',\n",
       " 'hearing',\n",
       " 'health',\n",
       " 'breaking',\n",
       " 'tuesday',\n",
       " 'white',\n",
       " 'help',\n",
       " 'force',\n",
       " 'debates2020',\n",
       " 'germany',\n",
       " 'see',\n",
       " 'report',\n",
       " 'country',\n",
       " 'government',\n",
       " 'home',\n",
       " 'pandemic',\n",
       " 'street',\n",
       " 'day',\n",
       " 'confirmation',\n",
       " 'french',\n",
       " 'week',\n",
       " 'u.s',\n",
       " 'need',\n",
       " 'high',\n",
       " 'vote',\n",
       " 'federal',\n",
       " 'lead',\n",
       " 'city',\n",
       " 'nominee',\n",
       " 'year',\n",
       " 'read',\n",
       " 'amy',\n",
       " 'barrett',\n",
       " 'deal',\n",
       " 'republican',\n",
       " 'star',\n",
       " 'old',\n",
       " 'write',\n",
       " 'state',\n",
       " 'lockdown',\n",
       " 'police',\n",
       " 'world',\n",
       " 'dog',\n",
       " 'twitter',\n",
       " 'reveal',\n",
       " 'administration',\n",
       " 'die',\n",
       " 'patient',\n",
       " 'cnn',\n",
       " 'tell',\n",
       " 'call',\n",
       " 'good',\n",
       " 'watch',\n",
       " 'second',\n",
       " 'child',\n",
       " 'house',\n",
       " 'school',\n",
       " 'scientist',\n",
       " 'voter',\n",
       " 'presidential',\n",
       " 'hit',\n",
       " 'elect',\n",
       " 'work',\n",
       " 'opinion',\n",
       " 'dec',\n",
       " 'big',\n",
       " 'nbcnewsthink',\n",
       " 'fox',\n",
       " 'europe',\n",
       " 'news']"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_feature_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:51.216400Z",
     "start_time": "2020-12-23T07:57:51.207782Z"
    }
   },
   "outputs": [],
   "source": [
    "dictOfWords = dict.fromkeys(unique_feature_list)\n",
    "key_names = []\n",
    "counter = 0\n",
    "for key in dictOfWords:\n",
    "    key_names.append(key)\n",
    "    dictOfWords[key] = counter \n",
    "    counter+=1\n",
    "\n",
    "lemma_list = list(df_prep.lemma_tokens)\n",
    "\n",
    "len(lemma_list)\n",
    "key_names.append('Not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:51.403662Z",
     "start_time": "2020-12-23T07:57:51.219791Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "159\n",
      "20796\n"
     ]
    }
   ],
   "source": [
    "feature_matrix = []\n",
    "feature_size = len(unique_feature_list) + 1\n",
    "\n",
    "lemma_list\n",
    "\n",
    "for array in lemma_list:\n",
    "    temp = [0]*feature_size\n",
    "    for k in array:\n",
    "        index = dictOfWords.get(k,feature_size-1)\n",
    "        temp[index] = 1\n",
    "    \n",
    "    feature_matrix.append(temp)\n",
    "\n",
    "print(feature_size)\n",
    "print(len(feature_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:52.438809Z",
     "start_time": "2020-12-23T07:57:51.406236Z"
    }
   },
   "outputs": [],
   "source": [
    "feature_df = pd.DataFrame(feature_matrix, index = df_prep.index, columns = key_names)\n",
    "feature_df = feature_df.drop(['Not found'], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:52.455720Z",
     "start_time": "2020-12-23T07:57:52.440892Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>positive</th>\n",
       "      <th>show</th>\n",
       "      <th>hunter</th>\n",
       "      <th>trump</th>\n",
       "      <th>russian</th>\n",
       "      <th>black</th>\n",
       "      <th>analysis</th>\n",
       "      <th>accord</th>\n",
       "      <th>biden</th>\n",
       "      <th>senate</th>\n",
       "      <th>...</th>\n",
       "      <th>hit</th>\n",
       "      <th>elect</th>\n",
       "      <th>work</th>\n",
       "      <th>opinion</th>\n",
       "      <th>dec</th>\n",
       "      <th>big</th>\n",
       "      <th>nbcnewsthink</th>\n",
       "      <th>fox</th>\n",
       "      <th>europe</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1340367556947943425</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340329823282016256</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 158 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     positive  show  hunter  trump  russian  black  analysis  \\\n",
       "1340367556947943425  0         0     0       0      0        0      0          \n",
       "1340329823282016256  0         0     0       0      0        0      0          \n",
       "\n",
       "                     accord  biden  senate  ...  hit  elect  work  opinion  \\\n",
       "1340367556947943425  0       0      0       ...  0    0      0     0         \n",
       "1340329823282016256  0       0      0       ...  0    0      0     0         \n",
       "\n",
       "                     dec  big  nbcnewsthink  fox  europe  news  \n",
       "1340367556947943425  0    0    0             0    0       0     \n",
       "1340329823282016256  0    0    0             0    0       0     \n",
       "\n",
       "[2 rows x 158 columns]"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:52.489405Z",
     "start_time": "2020-12-23T07:57:52.457934Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>agency</th>\n",
       "      <th>created_at</th>\n",
       "      <th>text</th>\n",
       "      <th>date</th>\n",
       "      <th>tokens</th>\n",
       "      <th>tokens_back_to_text</th>\n",
       "      <th>lemmas</th>\n",
       "      <th>lemmas_back_to_text</th>\n",
       "      <th>lemma_tokens</th>\n",
       "      <th>positive</th>\n",
       "      <th>...</th>\n",
       "      <th>hit</th>\n",
       "      <th>elect</th>\n",
       "      <th>work</th>\n",
       "      <th>opinion</th>\n",
       "      <th>dec</th>\n",
       "      <th>big</th>\n",
       "      <th>nbcnewsthink</th>\n",
       "      <th>fox</th>\n",
       "      <th>europe</th>\n",
       "      <th>news</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1340367556947943425</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-19 18:45:03</td>\n",
       "      <td>A $20,000 fee to change a wedding date? New bill aims to protect people from 'bad actor catering halls'</td>\n",
       "      <td>2020-12-19 00:00:00</td>\n",
       "      <td>[$20,000, fee, change, wedding, date?, new, aims, protect, people, 'bad, actor, catering, halls']</td>\n",
       "      <td>$20,000 fee change wedding date? new aims protect people 'bad actor catering halls'</td>\n",
       "      <td>[20,000, fee, change, wedding, date, new, aim, protect, people, bad, actor, catering, hall]</td>\n",
       "      <td>20,000 fee change wedding date new aim protect people bad actor catering hall</td>\n",
       "      <td>[20,000, fee, change, wedding, date, new, aim, protect, people, bad, actor, catering, hall]</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1340329823282016256</th>\n",
       "      <td>yahoonews</td>\n",
       "      <td>2020-12-19 16:15:06</td>\n",
       "      <td>U.S. intelligence agencies are increasingly focused on domestic extremists. Their latest target: Satanists</td>\n",
       "      <td>2020-12-19 00:00:00</td>\n",
       "      <td>[u.s., intelligence, agencies, increasingly, focused, domestic, extremists., latest, target:, satanists]</td>\n",
       "      <td>u.s. intelligence agencies increasingly focused domestic extremists. latest target: satanists</td>\n",
       "      <td>[u.s, intelligence, agency, increasingly, focus, domestic, extremist, late, target, satanist]</td>\n",
       "      <td>u.s intelligence agency increasingly focus domestic extremist late target satanist</td>\n",
       "      <td>[u.s, intelligence, agency, increasingly, focus, domestic, extremist, late, target, satanist]</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 167 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        agency          created_at  \\\n",
       "1340367556947943425  yahoonews 2020-12-19 18:45:03   \n",
       "1340329823282016256  yahoonews 2020-12-19 16:15:06   \n",
       "\n",
       "                                                                                                                             text  \\\n",
       "1340367556947943425  A $20,000 fee to change a wedding date? New bill aims to protect people from 'bad actor catering halls'        \n",
       "1340329823282016256  U.S. intelligence agencies are increasingly focused on domestic extremists. Their latest target: Satanists     \n",
       "\n",
       "                                    date  \\\n",
       "1340367556947943425  2020-12-19 00:00:00   \n",
       "1340329823282016256  2020-12-19 00:00:00   \n",
       "\n",
       "                                                                                                                       tokens  \\\n",
       "1340367556947943425  [$20,000, fee, change, wedding, date?, new, aims, protect, people, 'bad, actor, catering, halls']          \n",
       "1340329823282016256  [u.s., intelligence, agencies, increasingly, focused, domestic, extremists., latest, target:, satanists]   \n",
       "\n",
       "                                                                                               tokens_back_to_text  \\\n",
       "1340367556947943425  $20,000 fee change wedding date? new aims protect people 'bad actor catering halls'             \n",
       "1340329823282016256  u.s. intelligence agencies increasingly focused domestic extremists. latest target: satanists   \n",
       "\n",
       "                                                                                                            lemmas  \\\n",
       "1340367556947943425  [20,000, fee, change, wedding, date, new, aim, protect, people, bad, actor, catering, hall]     \n",
       "1340329823282016256  [u.s, intelligence, agency, increasingly, focus, domestic, extremist, late, target, satanist]   \n",
       "\n",
       "                                                                                    lemmas_back_to_text  \\\n",
       "1340367556947943425  20,000 fee change wedding date new aim protect people bad actor catering hall        \n",
       "1340329823282016256  u.s intelligence agency increasingly focus domestic extremist late target satanist   \n",
       "\n",
       "                                                                                                      lemma_tokens  \\\n",
       "1340367556947943425  [20,000, fee, change, wedding, date, new, aim, protect, people, bad, actor, catering, hall]     \n",
       "1340329823282016256  [u.s, intelligence, agency, increasingly, focus, domestic, extremist, late, target, satanist]   \n",
       "\n",
       "                     positive  ...  hit  elect  work  opinion  dec  big  \\\n",
       "1340367556947943425  0         ...  0    0      0     0        0    0     \n",
       "1340329823282016256  0         ...  0    0      0     0        0    0     \n",
       "\n",
       "                     nbcnewsthink  fox  europe  news  \n",
       "1340367556947943425  0             0    0       0     \n",
       "1340329823282016256  0             0    0       0     \n",
       "\n",
       "[2 rows x 167 columns]"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined_features = pd.concat([df_prep,feature_df],axis = 1)\n",
    "combined_features.head(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:52.499819Z",
     "start_time": "2020-12-23T07:57:52.491432Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wednesday', 'report', 'record', 'number', 'covid-19', 'case', 'hospitalization', 'death', 'ahead', 'key', 'meeting', 'country', 'second', 'covid-19', 'vaccine', 'green', 'light']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "covid-19    1\n",
       "case        1\n",
       "death       1\n",
       "record      1\n",
       "vaccine     1\n",
       "report      1\n",
       "country     1\n",
       "second      1\n",
       "Name: 1339480974917300224, dtype: int64"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test whether the feature creation works\n",
    "\n",
    "## entry number to test\n",
    "test = 899\n",
    "print(combined_features.iloc[test][6])\n",
    "feature_df.iloc[test][feature_df.iloc[test]>0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:52.505450Z",
     "start_time": "2020-12-23T07:57:52.502286Z"
    }
   },
   "outputs": [],
   "source": [
    "assert len(combined_features) == len(df_import) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-12-23T07:57:53.890963Z",
     "start_time": "2020-12-23T07:57:52.507318Z"
    }
   },
   "outputs": [],
   "source": [
    "## save file to drive\n",
    "combined_features.to_csv(\"bag of words_02.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
